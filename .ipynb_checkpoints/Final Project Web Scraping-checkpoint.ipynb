{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlencode\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "import os\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab527b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"PROXY_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = \"https://proxy.scrapeops.io/v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_list = [\n",
    "    # Chrome\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4571.63 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    },\n",
    "    # Firefox\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    },\n",
    "    # Safari\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    },\n",
    "    # Opera\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4571.63 Safari/537.36 OPR/78.0.4093.231\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e00d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_header(x):\n",
    "    \"\"\"Take as Input a list and return a random element from this\"\"\"\n",
    "    return random.choice(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a86282",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_links3 = {\n",
    " 'Santorini_July': 'https://www.booking.com/searchresults.html?ss=Santorini%2C+Greece&ssne=Naxos&ssne_untouched=Naxos&label=gen173nr-1FCAEoggI46AdIM1gEaFyIAQGYAQi4ARfIAQzYAQHoAQH4AQyIAgGoAgO4AvCy3aMGwAIB0gIkYzdkZjg5YzctNDdkNi00OTc3LTk5Y2QtMGM2ZDA2N2NhOTA32AIG4AIB&sid=4cc5dec2c68e6377819edd6ab66620de&aid=304142&lang=en-us&sb=1&src_elem=sb&src=searchresults&dest_id=1513&dest_type=region&ac_position=0&ac_click_type=b&ac_langcode=en&ac_suggestion_list_length=5&search_selected=true&search_pageview_id=89a05c05a49b01d2&ac_meta=GhA4OWEwNWMwNWE0OWIwMWQyIAAoATICZW46CVNhbnRvcmluaUAASgBQAA%3D%3D&checkin=2023-07-24&checkout=2023-07-30&group_adults=2&no_rooms=1&group_children=0',\n",
    " 'lefkada_July': 'https://www.booking.com/searchresults.html?ss=Lefkada%2C+Greece&ssne=Rhodes&ssne_untouched=Rhodes&label=gen173nr-1FCAEoggI46AdIM1gEaFyIAQGYAQi4ARfIAQzYAQHoAQH4AQyIAgGoAgO4AvCy3aMGwAIB0gIkYzdkZjg5YzctNDdkNi00OTc3LTk5Y2QtMGM2ZDA2N2NhOTA32AIG4AIB&sid=4cc5dec2c68e6377819edd6ab66620de&aid=304142&lang=en-us&sb=1&src_elem=sb&src=searchresults&dest_id=2805&dest_type=region&ac_position=0&ac_click_type=b&ac_langcode=en&ac_suggestion_list_length=5&search_selected=true&search_pageview_id=6c8d5c2ff6e20117&ac_meta=GhA2YzhkNWMyZmY2ZTIwMTE3IAAoATICZW46B0xlZmthZGFAAEoAUAA%3D&checkin=2023-07-24&checkout=2023-07-30&group_adults=2&no_rooms=1&group_children=0'}\n",
    "offset = \"&offset=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dictionary_links3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78feda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a3b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for csv_names,link in dictionary_links3.items():\n",
    "    print(f\"the link of the main page is {link}\")\n",
    "    try:\n",
    "        filter_url = link + offset\n",
    "        headers = get_random_header(headers_list)\n",
    "        response = requests.get(filter_url,headers=headers,timeout = 15)\n",
    "        soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "        page_number = int(soup.find(\"ol\",class_=\"a8b500abde\").find_all('li')[-1].text)\n",
    "        last_offset = page_number * 25\n",
    "        cols = [\n",
    "        'name', 'location', 'reviews', 'facilities' , 'price','size','rules','stars']\n",
    "\n",
    "\n",
    "        #name = pd.DataFrame(columns=cols)\n",
    "        df = pd.DataFrame(columns=cols)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    list_links = []\n",
    "    for page in tqdm(range(0,last_offset+1,25)):\n",
    "        try:\n",
    "            headers = get_random_header(headers_list)\n",
    "            #print(page)\n",
    "            # Url from different islands change manual\n",
    "            page_url = f\"{filter_url}{page}\"\n",
    "            #--------------------------------------------\n",
    "\n",
    "            #Use a proxy and take the respone and soup-------\n",
    "            proxy_params = {'api_key': api_key,'url': page_url}\n",
    "            response = requests.get(url=proxy,params=urlencode(proxy_params),timeout=120,headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #-----------------------------------------------\n",
    "\n",
    "            #Scrape the links from every listing in booking\n",
    "            block = soup.find(\"div\",class_=\"dcf496a7b9\")\n",
    "            links = block.find_all(\"h3\",class_=\"a4225678b2\")\n",
    "\n",
    "            #Looping and append in the list named list_links\n",
    "            for link in links:\n",
    "                list_links.append(link.find(\"a\")[\"href\"])\n",
    "\n",
    "            #Sleep time random between 0.8-1.3 sec for each page\n",
    "            #time.sleep(random.uniform(0.8, 1.3))   -----------Maybe Not use-------------\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    #==========================================================\n",
    "    \n",
    "    \n",
    "    for link in tqdm(list_links):\n",
    "        headers = get_random_header(headers_list)\n",
    "        \n",
    "    \n",
    "    # Using the proxy and get the respone\n",
    "        proxy_params = {'api_key': api_key,'url': link}\n",
    "        response = requests.get(url=proxy,params=urlencode(proxy_params),timeout=120,headers=headers)\n",
    "        #-------------------------------------\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser') # Take the html parser with BeautifulSoup and call the var \"soup\"\n",
    "            try:\n",
    "                name = soup.find(\"div\",{\"data-capla-component\": \"b-property-web-property-page/PropertyHeaderName\"}).get_text()\n",
    "            except:\n",
    "                name\n",
    "            try:\n",
    "                location = soup.find(\"p\",class_=\"address address_clean\").find(\"span\",class_=\"hp_address_subtitle js-hp_address_subtitle jq_tooltip\").get_text().strip()\n",
    "            except:\n",
    "                location\n",
    "            try:\n",
    "                reviews = soup.find(\"div\",{\"class\",\"hp-gallery-review\"}).find(\"div\",{\"id\":\"js--hp-gallery-scorecard\"}).find(\"div\",{\"class\":\"a1b3f50dcd be36d14cea cbb2d85c33 db7f07f643 d19ba76520 d02f1578ba d17b3fe5e2\"}).get_text()\n",
    "            except:\n",
    "                reviews = np.nan\n",
    "            try:\n",
    "                lst = []\n",
    "                pop= soup.find(\"div\",{\"data-testid\" :\"facility-list-most-popular-facilities\"}).find_all(\"div\",{\"class\":\"a815ec762e ab06168e66\"})\n",
    "                for l in pop:\n",
    "                    lst.append(l.find(\"span\",class_=\"db312485ba\").get_text())\n",
    "                facilities = lst\n",
    "            except:\n",
    "                facilities = np.nan\n",
    "            try:\n",
    "                #cell = soup.find_all(\"td\",class_=\"hp-price-left-align hprt-table-cell hprt-table-cell-price\")[0]\n",
    "                #price = cell.find(\"span\",class_=\"bui-u-sr-only\").text.split()[-1]\n",
    "                price = soup.find_all(\"span\",class_=\"bui-u-sr-only\")[1]\n",
    "            except:\n",
    "                price= np.nan\n",
    "            try:\n",
    "                size = soup.find(\"div\",{\"data-name-en\":\"room size\"}).text\n",
    "            except:\n",
    "                size = np.nan\n",
    "            try:\n",
    "                lst = []\n",
    "                rules = soup.find_all(\"div\",class_=\"description description--house-rule\")\n",
    "                rules_list = list(rules)\n",
    "                for r in rules_list:\n",
    "                    lst.append(r.text.strip())\n",
    "\n",
    "                rules_append = lst\n",
    "            except:\n",
    "                rules_append = np.nan\n",
    "            try:\n",
    "                stars = len(soup.find(\"span\",{\"class\":\"fbb11b26f5 e23c0b1d74\"}).find_all(\"span\",{\"class\":\"b6dc9a9e69 adc357e4f1 fe621d6382\"}))\n",
    "            except:\n",
    "                stars = np.nan\n",
    "\n",
    "         # Create a dic with the elements \n",
    "\n",
    "            temp = {\n",
    "                'name': name,\n",
    "                'location': location,\n",
    "                'reviews': reviews,\n",
    "                'facilities': facilities,\n",
    "                'price':price,\n",
    "                'size':size,\n",
    "                'rules':rules_append,\n",
    "                'stars':stars,\n",
    "                'link':link\n",
    "\n",
    "            }\n",
    "\n",
    "            #Input in the DataFrame\n",
    "            #exec(f\"{name} = name.append(temp, ignore_index=True)\")\n",
    "            df= df.append(temp, ignore_index=True)\n",
    "            #-------------------------\n",
    "\n",
    "            #time.sleep(random.uniform(0.8, 1.3)) # Loopin every (0.8, 1.3) random -----MAYBE NOT USE----------------\n",
    "\n",
    "        else:\n",
    "            print('website blocked')\n",
    "        \n",
    "    df.to_csv(f\"{csv_names}.csv\",index=False)\n",
    "    print(f\"File {csv_names} exported successfuly\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082ee2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7479b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ddeb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1d320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d11148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664fdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4e980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
